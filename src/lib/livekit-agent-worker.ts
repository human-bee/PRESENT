#!/usr/bin/env node
/**
 * Tambo Voice Agent - LiveKit Agent JS Implementation
 * 
 * A voice-enabled AI agent using LiveKit's real-time communication
 * infrastructure and OpenAI's Realtime API for natural conversations.
 */

import { config } from 'dotenv';
import { join } from 'path';

// Load environment variables from .env.local
config({ path: join(process.cwd(), '.env.local') });

import { defineAgent, JobContext, cli, WorkerOptions, multimodal } from '@livekit/agents';
import { RoomEvent, Track } from 'livekit-client';
import * as openai from '@livekit/agents-plugin-openai';
import { DecisionEngine } from './decision-engine';

console.log('üöÄ Starting Tambo Voice Agent Worker...');
console.log('üîß Environment Check:');
console.log(`  - OpenAI API Key: ${process.env.OPENAI_API_KEY ? '‚úÖ Present' : '‚ùå Missing'}`);
console.log(`  - LiveKit API Key: ${process.env.LIVEKIT_API_KEY ? '‚úÖ Present' : '‚ùå Missing'}`);
console.log(`  - LiveKit URL: ${process.env.LIVEKIT_URL || process.env.NEXT_PUBLIC_LK_SERVER_URL || '‚ùå Missing'}`);

export default defineAgent({
  entry: async (job: JobContext) => {
    console.log(`üéâ [Agent] Job received! Joining room: ${job.room.name}`);
    console.log(`üìä [Agent] Room details:`, {
      roomName: job.room.name,
      remoteParticipantsCount: job.room.remoteParticipants.size,
      metadata: job.room.metadata || 'none',
      timestamp: new Date().toISOString()
    });
    
    await job.connect();
    console.log('‚úÖ [Agent] Successfully connected to room!');
    
    // --- AUDIO DEBUG: Enhanced participant tracking ---------------------
    console.log('[TRACE] üë• Participants when agent joined:',
                [...job.room.remoteParticipants.values()].map(p => p.identity));
    
    // Check room state every few seconds to see who's really there
    const checkRoomState = () => {
      console.log('\n[TRACE] üîç Current room state:');
      console.log(`  üìä Total participants: ${job.room.remoteParticipants.size}`);
      for (const participant of job.room.remoteParticipants.values()) {
        console.log(`  üë§ ${participant.identity}:`);
        console.log(`     - Track publications: ${participant.trackPublications.size}`);
        for (const pub of participant.trackPublications.values()) {
          console.log(`     - ${pub.kind} track`);
        }
      }
      console.log('');
    };
    
    // Check immediately and then every 30 seconds (instead of 10)
    setTimeout(checkRoomState, 1000);
    setInterval(checkRoomState, 30000);
    // ----------------------------------------------------------------

    // 1Ô∏è‚É£  every time a participant connects/disconnects
    job.room
      .on(RoomEvent.ParticipantConnected, p =>
          console.log('[TRACE] participant connected', p.identity))
      .on(RoomEvent.ParticipantDisconnected, p =>
          console.log('[TRACE] participant disconnected', p.identity));

    // 2Ô∏è‚É£  every time a remote audio publication appears
    job.room.on(RoomEvent.TrackPublished, async (pub: any, p: any) => {
      console.log('[TRACE] trackPublished',
                  p.identity, pub.name || pub.trackName, pub.kind);
      if (pub.kind === Track.Kind.Audio || pub.kind === 'audio') {
        try {
          await pub.setSubscribed(true);
          console.log(`[SUCCESS] subscribed to ${p.identity}'s audio track`);
        } catch (err) {
          console.error('[ERROR] could not subscribe to', p.identity, err);
        }
      }
    });
    
    // Track the most recently active speaker in the room with enhanced debugging
    let lastActiveSpeaker: string | null = null;
    job.room.on('activeSpeakersChanged', (speakers) => {
      console.log(`üó£Ô∏è [Agent] Active speakers changed:`, {
        count: speakers.length,
        identities: speakers.map(s => s.identity),
        audioLevels: speakers.map(s => ({ identity: s.identity, level: (s as any).audioLevel || 0 }))
      });
      
      if (speakers.length > 0) {
        const previousSpeaker = lastActiveSpeaker;
        lastActiveSpeaker = speakers[0].identity;
        console.log(`üéØ [Agent] Primary speaker changed: ${previousSpeaker} ‚Üí ${lastActiveSpeaker}`);
      } else {
        console.log(`üîá [Agent] No active speakers detected`);
      }
    });
    
    console.log('üß† [Agent] Initializing OpenAI Realtime model...');
    
    // Create the multimodal agent using OpenAI Realtime API
    // Note: Tools will be handled through OpenAI's native function calling mechanism
    const model = new openai.realtime.RealtimeModel({
      instructions: `You are Tambo Voice Agent, a helpful AI assistant integrated with a powerful UI generation system.
        
        CRITICAL: Always respond with TEXT ONLY. Never use audio responses. All your responses should be in text format.
        
        IMPORTANT: When users ask for UI components, timers, or visual elements, DO NOT repeat their request back as text. The UI generation is handled automatically when they speak.
        
        You have access to these tools:
        - generate_ui_component: Create ANY UI component (timers, charts, buttons, forms, etc.) - Tambo knows about all available components
        - youtube_search: Search and display YouTube videos
        - mcp_tool: Access external tools via Model Context Protocol
        - ui_update: Update existing UI components (MUST call list_components first!)
        - list_components: List all current UI components to get their IDs
        - respond_with_voice: Speak responses when appropriate
        - do_nothing: When no action is needed
        
        Always respond with text for:
        - Answering questions
        - Providing explanations
        - Casual conversation
        - Confirming actions that YOU perform
        
        DO NOT use voice to repeat UI requests like "Create a timer" or "Show me a chart" - these are handled automatically by the system.
        
        Remember: TEXT RESPONSES ONLY, even though you can hear audio input.`,
      model: 'gpt-4o-realtime-preview',
      modalities: ['text']
    });
    
    console.log('üéôÔ∏è [Agent] Starting multimodal agent...');
    
    // Initialize Decision Engine
    const decisionEngine = new DecisionEngine(process.env.OPENAI_API_KEY || '');
    console.log('üß† [Agent] Decision Engine initialized');
    
    // Configure agent to accept text responses when using tools
    const agent = new multimodal.MultimodalAgent({ 
      model,
      // Use a very high limit so the built-in check never throws
      maxTextResponseRetries: Number.MAX_SAFE_INTEGER
    });
    
    // Start the agent session
    const session = await agent
      .start(job.room)
      .then(session => {
        console.log('‚úÖ [Agent] Multimodal agent started successfully!');
        
        // Note: Tools are configured through OpenAI Realtime API's native function calling
        // The session.on('response_function_call_completed') handler below processes tool calls
        console.log('üîß [Agent] Using OpenAI Realtime API native function calling');
        
        // Send welcome message after agent is ready
        setTimeout(() => {
          const welcomeData = JSON.stringify({
            type: 'live_transcription',
            text: 'ü§ñ Tambo Voice Agent connected! I can hear you and respond naturally. Just speak!',
            speaker: 'tambo-voice-agent',
            timestamp: Date.now(),
            is_final: true,
          });
          
          job.room.localParticipant?.publishData(
            new TextEncoder().encode(welcomeData),
            { reliable: true, topic: 'transcription' }
          );
          console.log('üì§ [Agent] Welcome message sent');
        }, 1000);
        
        // Override recoverFromTextResponse to turn it into a no-op so text responses are fine
        (session as unknown as { recoverFromTextResponse: () => void }).recoverFromTextResponse = () => {};
        
        return session;
      })
      .catch(error => {
        console.error('‚ùå [Agent] Failed to start multimodal agent:', error);
        throw error;
      });
    
    // Handle text-only responses from the model
    session.on('response_content_done', (evt: { contentType: string; text: string; itemId: string }) => {
      if (evt.contentType === 'text') {
        console.log(`üìù [Agent] Text-only response received: "${evt.text}"`);
        
        // Only log the text response - don't send it as a tool call
        // The user's actual speech is already being sent to Tambo
        
        // Send as transcription for display
        const transcriptionData = JSON.stringify({
          type: 'live_transcription',
          text: evt.text,
          speaker: 'tambo-voice-agent',
          timestamp: Date.now(),
          is_final: true,
        });
        
        job.room.localParticipant?.publishData(
          new TextEncoder().encode(transcriptionData),
          { reliable: true, topic: 'transcription' }
        );
      }
    });
    
    // Handle function calls from the agent
    session.on('response_function_call_completed', async (evt: { 
      call_id: string; 
      name: string; 
      arguments: string; 
    }) => {
      console.log(`üîß [Agent] Function call completed: ${evt.name}`);
      try {
        const args = JSON.parse(evt.arguments);
        
        // Instead of executing directly, send to ToolDispatcher
        const toolCallEvent = {
          id: evt.call_id,
          roomId: job.room.name || 'unknown',
          type: 'tool_call',
          payload: {
            tool: evt.name,
            params: args,
            context: {
              source: 'voice',
              timestamp: Date.now(),
            }
          },
          timestamp: Date.now(),
          source: 'voice' as const,
        };
        
        // Publish to tool dispatcher
        await job.room.localParticipant?.publishData(
          new TextEncoder().encode(JSON.stringify(toolCallEvent)),
          { reliable: true, topic: 'tool_call' }
        );
        
        console.log(`‚úÖ [Agent] Tool call dispatched:`, evt.name);
        
        // For now, return a placeholder result to keep the session going
        // The actual result will come from the tool dispatcher
        session.conversation.item.create({
          type: 'function_call_output',
          call_id: evt.call_id,
          output: JSON.stringify({ 
            status: 'DISPATCHED', 
            message: 'Tool call sent to dispatcher',
            timestamp: Date.now()
          })
        });
        
      } catch (error) {
        console.error(`‚ùå [Agent] Function call error:`, error);
        // Submit error result
        session.conversation.item.create({
          type: 'function_call_output',
          call_id: evt.call_id,
          output: JSON.stringify({ status: 'ERROR', message: String(error) })
        });
      }
    });
    
    // Set up decision engine callback to handle Tambo forwarding
    decisionEngine.onDecision(async (decision, participantId, originalText) => {
      if (!decision.should_send) {
        console.log(`‚è∏Ô∏è [Agent] Filtered out: "${originalText}" (confidence: ${decision.confidence}%)`);
        return;
      }

      console.log(`‚úÖ [Agent] Sending to Tambo: "${decision.summary}" (confidence: ${decision.confidence}%)`);
      
      // Ensure we have a valid prompt - fallback to original text if summary is empty
      const prompt = decision.summary && decision.summary.trim() ? decision.summary : originalText;
      
      // Final safety check - should never happen but just in case
      if (!prompt || prompt.trim() === '') {
        console.error(`‚ùå [Agent] Empty prompt detected! Decision:`, decision, `OriginalText: "${originalText}"`);
        return;
      }
      
      console.log(`üîç [Agent] Debug - prompt: "${prompt}", originalText: "${originalText}"`);
      
      // Determine the appropriate tool based on intent
      const tool = decision.intent === 'youtube_search' ? 'youtube_search' : 'generate_ui_component';
      
      // Forward the summary to Tambo with enhanced context
      const toolCallEvent = {
        id: `smart-speech-${Date.now()}`,
        roomId: job.room.name || 'unknown',
        type: 'tool_call',
        payload: {
          tool: tool,
          params: {
            prompt: prompt,
            task_prompt: prompt,
            query: decision.intent === 'youtube_search' ? (decision.structuredContext?.rawQuery || prompt) : undefined
          },
          context: {
            source: 'voice',
            timestamp: Date.now(),
            transcript: originalText,
            summary: decision.summary,
            speaker: participantId,
            confidence: decision.confidence,
            reason: decision.reason,
            intent: decision.intent,
            structuredContext: decision.structuredContext
          }
        },
        timestamp: Date.now(),
        source: 'voice' as const,
      };

      console.log(`üì§ [Agent] Tool call event:`, JSON.stringify(toolCallEvent, null, 2));

      job.room.localParticipant?.publishData(
        new TextEncoder().encode(JSON.stringify(toolCallEvent)),
        { reliable: true, topic: 'tool_call' }
      );
    });

    // Track participant rotation for better attribution (now using time-based rotation)

    // Subscribe to transcription events from all participants
    session.on('input_speech_transcription_completed', async (evt: { transcript: string }) => {
      console.log(`üë§ [Agent] Speech transcribed: "${evt.transcript}"`);
      
      // Debug current room state
      const participants = Array.from(job.room.remoteParticipants.values());
      console.log(`üîç [Agent] Room state during transcription:`, {
        totalParticipants: participants.length,
        participantIdentities: participants.map(p => p.identity),
        lastActiveSpeaker: lastActiveSpeaker
      });

      // Determine the speaker using LiveKit's active speaker info if available
      let speakerId = lastActiveSpeaker || 'unknown-speaker';
      let attributionMethod = 'active-speaker';

      if (!lastActiveSpeaker && job.room.remoteParticipants.size > 0) {
        if (participants.length > 1) {
          // Slower rotation - change every 10 seconds instead of every transcription
          const slowRotationIndex = Math.floor(Date.now() / 10000) % participants.length;
          speakerId = participants[slowRotationIndex]?.identity || 'participant-1';
          attributionMethod = 'slow-rotation';
        } else {
          speakerId = participants[0]?.identity || 'participant-1';
          attributionMethod = 'single-participant';
        }
      }

      console.log(`üó£Ô∏è [Agent] Speech attribution:`, {
        transcript: evt.transcript,
        attributedTo: speakerId,
        method: attributionMethod,
        allParticipants: participants.map(p => p.identity)
      });
      
      // Create a unique key to check for duplicates
      const transcriptionKey = `${evt.transcript}-${Math.floor(Date.now() / 1000)}`;
      
      // Only process if this transcription hasn't been processed recently
      if (!processedTranscriptions.has(transcriptionKey)) {
        processedTranscriptions.add(transcriptionKey);
        
        // Clean up old entries after 5 seconds
        setTimeout(() => processedTranscriptions.delete(transcriptionKey), 5000);
        
        // Send transcription to frontend for display
        const transcriptionData = JSON.stringify({
          type: 'live_transcription',
          text: evt.transcript,
          speaker: speakerId,
          timestamp: Date.now(),
          is_final: true,
          agentId: job.room.localParticipant?.identity // Include agent ID for debugging
        });
        
        job.room.localParticipant?.publishData(
          new TextEncoder().encode(transcriptionData),
          { reliable: true, topic: 'transcription' }
        );
        
        // Process through decision engine with participant ID
        await decisionEngine.processTranscript(evt.transcript, speakerId);
      } else {
        console.log(`‚è≠Ô∏è [Agent] Skipping duplicate transcription: "${evt.transcript}"`);
      }
    });

    // Log participant connections for audio tracking
    job.room.on('participantConnected', (participant) => {
      console.log(`üë§ [Agent] Participant joined: ${participant.identity} - will capture their audio via OpenAI Realtime`);
    });
    
    job.room.on('participantDisconnected', (participant) => {
      console.log(`üëã [Agent] Participant left: ${participant.identity}`);
    });
    
    // Log when agent responds
    session.on('response_content_completed', (evt: { content_type: string; text: string }) => {
      if (evt.content_type === 'text') {
        console.log(`ü§ñ [Agent] Assistant said: "${evt.text}"`);
        
        // Send agent response to frontend  
        const responseData = JSON.stringify({
          type: 'live_transcription',
          text: evt.text,
          speaker: 'tambo-voice-agent',
          timestamp: Date.now(),
          is_final: true,
        });
        
        job.room.localParticipant?.publishData(
          new TextEncoder().encode(responseData),
          { reliable: true, topic: 'transcription' }
        );
      }
    });
    
    // Track processed transcriptions to avoid duplicates
    const processedTranscriptions = new Set<string>();
    
    // Listen for tool results from the frontend ToolDispatcher
    // This replaces the old RPC approach with data channel events
    const handleDataReceived = (data: Uint8Array, participant?: unknown, kind?: unknown, topic?: string) => {
      try {
        const message = JSON.parse(new TextDecoder().decode(data));
        
        // Handle tool results
        if (topic === 'tool_result' && message.type === 'tool_result') {
          console.log(`‚úÖ [Agent] Tool result received:`, {
            toolCallId: message.toolCallId,
            executionTime: message.executionTime,
            hasResult: !!message.result
          });
          // Agent can process tool results if needed
        }
        
        // Handle tool errors
        if (topic === 'tool_error' && message.type === 'tool_error') {
          console.error(`‚ùå [Agent] Tool error received:`, {
            toolCallId: message.toolCallId,
            error: message.error
          });
          // Agent can handle tool errors if needed
        }
        
        // Check for transcriptions from other agents to avoid duplicates
        if (topic === 'transcription' && message.type === 'live_transcription' && message.speaker?.startsWith('agent-')) {
          const transcriptionKey = `${message.text}-${message.timestamp}`;
          processedTranscriptions.add(transcriptionKey);
        }
      } catch {
        // Not all data messages are JSON, so this is expected sometimes
      }
    };
    
    // Subscribe to tool result topics
    job.room.on('dataReceived', handleDataReceived);
    

    
    // Handle data messages from frontend
    job.room.on('dataReceived', (data, participant) => {
      try {
        const message = JSON.parse(new TextDecoder().decode(data));
        console.log(`üì® [Agent] Data received from ${participant?.identity}:`, {
          type: message.type,
          hasContent: !!message.content || !!message.text
        });
        
        // Handle text-based messages (voice is handled by the multimodal agent)
        if (message.type === 'user_message' || message.type === 'chat_message') {
          console.log(`üí¨ [Agent] User text message: "${message.content || message.text}"`);
          
          // For text messages, we acknowledge but remind user this is a voice agent
          const responseData = JSON.stringify({
            type: 'live_transcription',
            text: `I received your text: "${message.content || message.text}". For the best experience, try speaking to me!`,
            speaker: 'tambo-voice-agent', 
            timestamp: Date.now(),
            is_final: true,
          });
          
          job.room.localParticipant?.publishData(
            new TextEncoder().encode(responseData),
            { reliable: true, topic: 'transcription' }
          );
        }
      } catch (error) {
        console.error('‚ùå [Agent] Error processing data message:', error);
      }
    });
    
    // Clean up decision engine on disconnect
    job.room.on('disconnected', () => {
      console.log('üßπ [Agent] Cleaning up decision engine...');
      decisionEngine.clearAllBuffers();
    });
    
    console.log('üéØ [Agent] Fully initialized with smart decision engine');

    // Ensure we receive audio from every participant
    const subscribeToAudio = async (publication: any, participant: any) => {
      if (publication.kind === Track.Kind.Audio || publication.kind === 'audio') {
        try {
          await publication.setSubscribed(true);
          console.log(`üîä [Agent] Subscribed to ${participant.identity}'s audio track`);
        } catch (err) {
          console.error(`‚ùå [Agent] Failed to subscribe to ${participant.identity}'s audio track:`, err);
        }
      }
    };

    // Subscribe to future tracks
    job.room.on(RoomEvent.TrackPublished, subscribeToAudio as any);

    // Subscribe to already-published tracks
    for (const participant of job.room.remoteParticipants.values()) {
      for (const publication of participant.trackPublications.values()) {
        await subscribeToAudio(publication, participant);
      }
    }

    // Add auto-cleanup logic after participant connection/disconnection handlers
    // Disconnect the agent if no human participants remain in the room for >10 s
    const isHuman = (p: { identity: string; metadata?: string }) => {
      const id = p.identity.toLowerCase();
      const meta = (p.metadata || '').toLowerCase();
      return !(
        id.includes('agent') ||
        id.includes('bot') ||
        id.includes('ai') ||
        id.startsWith('tambo-voice-agent') ||
        meta.includes('agent') ||
        meta.includes('type":"agent')
      );
    };

    let disconnectTimer: NodeJS.Timeout | null = null;

    const scheduleOrCancelDisconnect = () => {
      const humanParticipants = Array.from(job.room.remoteParticipants.values()).filter(isHuman);

      if (humanParticipants.length === 0) {
        if (!disconnectTimer) {
          console.log('üßπ [Agent] Room is empty of humans. Scheduling disconnect in 10 s‚Ä¶');
          disconnectTimer = setTimeout(() => {
            console.log('üîå [Agent] Disconnecting ‚Äì no human participants remained for 10 s');
            // Cleanly disconnect the agent and exit the process so the worker shuts down
            try {
              job.room.disconnect();
            } catch (err) {
              console.error('‚ö†Ô∏è [Agent] Error during disconnect:', err);
            }
            process.exit(0);
          }, 10000);
        }
      } else if (disconnectTimer) {
        // Humans have (re)joined ‚Üí cancel pending shutdown
        clearTimeout(disconnectTimer);
        disconnectTimer = null;
        console.log('üîÑ [Agent] Human participant detected ‚Äì canceling pending disconnect');
      }
    };

    // Monitor participant changes to trigger the above logic
    job.room.on(RoomEvent.ParticipantConnected, scheduleOrCancelDisconnect);
    job.room.on(RoomEvent.ParticipantDisconnected, scheduleOrCancelDisconnect);

    // Run at start in case the agent was dispatched into an already-empty room
    scheduleOrCancelDisconnect();
  },
});

// Use the CLI runner if this file is being run directly
if (import.meta.url.startsWith('file:') && process.argv[1].endsWith('livekit-agent-worker.ts')) {
  console.log('üé¨ [Agent] Starting worker...');
  
  // Configure worker options WITH agent name for automatic dispatch
  const workerOptions = new WorkerOptions({ 
    agent: process.argv[1], // Path to this agent file
    agentName: 'tambo-voice-agent', // Register under this name so dispatcher can find it
  });
  
  console.log('üîß [Agent] Worker configured for automatic dispatch');
  console.log('üì° [Agent] Connecting to LiveKit Cloud...');
  console.log('üåê [Agent] LiveKit URL:', process.env.LIVEKIT_URL || process.env.NEXT_PUBLIC_LK_SERVER_URL);
  
  cli.runApp(workerOptions);
} 